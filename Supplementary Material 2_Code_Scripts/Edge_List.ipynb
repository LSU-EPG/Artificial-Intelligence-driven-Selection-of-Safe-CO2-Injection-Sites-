{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from itertools import combinations\n",
    "\n",
    "# Update the file path to match your local directory\n",
    "file_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Database_with_Spatial_Analysis_final.xlsx\"\n",
    "\n",
    "# Load the dataset (modify sheet name if necessary)\n",
    "df = pd.read_excel(file_path, sheet_name=\"Sheet1\")  # Change sheet name if needed\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = {'ID', 'Latitude', 'Longitude', 'Class'}\n",
    "if not required_columns.issubset(df.columns):\n",
    "    raise ValueError(f\"Missing required columns: {required_columns - set(df.columns)}\")\n",
    "\n",
    "print(\"Columns found. Proceeding with filtering and distance calculations...\")\n",
    "\n",
    "# Filter only rows where Class is 'VI'\n",
    "df_filtered = df[df['Class'] == 'VI']\n",
    "\n",
    "# Create an edge list\n",
    "edge_list = []\n",
    "\n",
    "# Compute all-to-all distances for filtered rows only\n",
    "for (id1, lat1, lon1), (id2, lat2, lon2) in combinations(df_filtered[['ID', 'Latitude', 'Longitude']].values, 2):\n",
    "    distance_km = geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "    edge_list.append([id1, id2, distance_km, \"Sink\"])\n",
    "    edge_list.append([id2, id1, distance_km, \"Sink\"])  # Add reverse direction\n",
    "\n",
    "# Convert to DataFrame\n",
    "edge_df = pd.DataFrame(edge_list, columns=[\"From\", \"To\", \"Dist_Km\", \"Type\"])\n",
    "\n",
    "# Save to CSV\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_node_edgelist_filtered.csv\"\n",
    "edge_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered edge list with distances saved as '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca8703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "from itertools import product\n",
    "\n",
    "# File paths\n",
    "sink_nodes_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Output_Geospatial_Analysis_Final_filled.xlsx\"\n",
    "candidate_nodes_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project1\\Databases_Sreekanta\\Databases_Sreekanta\\random_points1000.shp\"\n",
    "class_ii_shut_in_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Databases_Sreekanta\\LA_NETL_SONRIS_VI_Dry.shp\"\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_candidate_edgelist_filtered.csv\"\n",
    "\n",
    "# Load sink nodes from Excel\n",
    "df_all = pd.read_excel(sink_nodes_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "# Load candidate nodes from shapefiles\n",
    "gdf_candidates_1 = gpd.read_file(candidate_nodes_path)\n",
    "gdf_candidates_2 = gpd.read_file(class_ii_shut_in_path)\n",
    "\n",
    "# Convert geometries to WGS84 and extract lat/lon\n",
    "for gdf in [gdf_candidates_1, gdf_candidates_2]:\n",
    "    gdf.to_crs(epsg=4326, inplace=True)\n",
    "    gdf['Latitude'] = gdf.geometry.y\n",
    "    gdf['Longitude'] = gdf.geometry.x\n",
    "\n",
    "# Filter Class VI for From nodes\n",
    "df_from = df_all[df_all['Class'] == 'VI'][['ID', 'Latitude', 'Longitude']]\n",
    "\n",
    "# Filter candidates from all 3 sources based on class\n",
    "valid_to_classes = {\"SHUT-IN DRY HOLE -FUTURE UTILITY\", \"RANDOM EXPERT GUIDED\"}\n",
    "df_to = df_all[df_all['Class'].isin(valid_to_classes)][['ID', 'Latitude', 'Longitude']]\n",
    "df_to = pd.concat([\n",
    "    df_to,\n",
    "    gdf_candidates_1[gdf_candidates_1['Class'].isin(valid_to_classes)][['ID', 'Latitude', 'Longitude']],\n",
    "    gdf_candidates_2[gdf_candidates_2['Class'].isin(valid_to_classes)][['ID', 'Latitude', 'Longitude']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# Sanity check on coordinate ranges\n",
    "df_from = df_from[df_from['Latitude'].between(-90, 90) & df_from['Longitude'].between(-180, 180)]\n",
    "df_to = df_to[df_to['Latitude'].between(-90, 90) & df_to['Longitude'].between(-180, 180)]\n",
    "\n",
    "# Check if we have valid data\n",
    "if df_from.empty or df_to.empty:\n",
    "    print(\"No matching 'From' (Class VI) or 'To' (candidate) nodes found.\")\n",
    "    exit()\n",
    "\n",
    "# Compute geodesic distances\n",
    "edge_list = []\n",
    "for (from_id, from_lat, from_lon), (to_id, to_lat, to_lon) in product(\n",
    "    df_from.values, df_to.values\n",
    "):\n",
    "    distance_km = geodesic((from_lat, from_lon), (to_lat, to_lon)).kilometers\n",
    "    edge_list.append([from_id, to_id, distance_km, \"Candidate\"])\n",
    "\n",
    "# Save edge list\n",
    "edge_df = pd.DataFrame(edge_list, columns=[\"From\", \"To\", \"Dist_Km\", \"Type\"])\n",
    "edge_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Filtered edge list with distances saved as '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "from itertools import product\n",
    "\n",
    "# ðŸ“‚ File Paths\n",
    "sink_nodes_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Output_Geospatial_Analysis_Final_filled.xlsx\"\n",
    "population_nodes_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Databases_Sreekanta\\LA_Pop_Cluster.shp\"\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_candidate_population_edgelist.csv\"\n",
    "\n",
    "# ðŸ“Œ Load sink/candidate nodes\n",
    "df_sinks = pd.read_excel(sink_nodes_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "# ðŸ“Œ Load census tract centroid data (FIPS, population)\n",
    "gdf_population = gpd.read_file(population_nodes_path)\n",
    "\n",
    "# Check geometry types before extracting coordinates\n",
    "print(\"Geometry types in population dataset:\", gdf_population.geom_type.unique())\n",
    "\n",
    "# Convert to standard latitude/longitude (EPSG:4326)\n",
    "gdf_population = gdf_population.to_crs(epsg=4326)\n",
    "\n",
    "# Convert polygons to centroids if needed\n",
    "if gdf_population.geom_type.iloc[0] != \"Point\":\n",
    "    print(\"Converting polygons to centroids...\")\n",
    "    gdf_population[\"geometry\"] = gdf_population.centroid\n",
    "\n",
    "# Extract latitude and longitude\n",
    "gdf_population['Latitude'] = gdf_population.geometry.y\n",
    "gdf_population['Longitude'] = gdf_population.geometry.x\n",
    "\n",
    "# Required columns check\n",
    "required_columns_pop = {'FIPS', 'Latitude', 'Longitude', 'POP_SQMI', 'POPULATION'}\n",
    "if not required_columns_pop.issubset(gdf_population.columns):\n",
    "    raise ValueError(f\"Missing columns in population dataset: {required_columns_pop - set(gdf_population.columns)}\")\n",
    "\n",
    "# Ensure latitude/longitude values are valid\n",
    "df_sinks = df_sinks[(df_sinks['Latitude'].between(-90, 90)) & (df_sinks['Longitude'].between(-180, 180))]\n",
    "gdf_population = gdf_population[(gdf_population['Latitude'].between(-90, 90)) & (gdf_population['Longitude'].between(-180, 180))]\n",
    "\n",
    "# Define 30-mile radius filter function\n",
    "def within_30_miles(lat1, lon1, lat2, lon2):\n",
    "    return geodesic((lat1, lon1), (lat2, lon2)).miles <= 30\n",
    "\n",
    "# Create edge list\n",
    "edge_list = []\n",
    "\n",
    "# Compute distances from each sink/candidate to census tract centroid\n",
    "for (node_id, node_lat, node_lon), (fips, pop_lat, pop_lon, tract_densqmi, tot_pop) in product(\n",
    "    df_sinks[['ID', 'Latitude', 'Longitude']].values,\n",
    "    gdf_population[['FIPS', 'Latitude', 'Longitude', 'POP_SQMI', 'POPULATION']].values\n",
    "):\n",
    "    if within_30_miles(node_lat, node_lon, pop_lat, pop_lon):\n",
    "        distance_km = geodesic((node_lat, node_lon), (pop_lat, pop_lon)).kilometers\n",
    "        edge_list.append([node_id, fips, distance_km, tract_densqmi, tot_pop, \"PopulationCluster\"])\n",
    "\n",
    "# Convert to DataFrame\n",
    "edge_df = pd.DataFrame(edge_list, columns=[\"From\", \"To\", \"Dist_Km\", \"Tract_Densqmi\", \"TotPop\", \"Type\"])\n",
    "\n",
    "# Save to CSV\n",
    "edge_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered edge list with census tract distances saved as '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5594b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "from itertools import product\n",
    "\n",
    "# ðŸ“‚ File Paths\n",
    "ejs_data_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Output_Geospatial_Analysis_Final_filled.xlsx\"\n",
    "source_data_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Databases_Sreekanta\\LA_CO2_Source.shp\"\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_candidate_source_edgelist.csv\"\n",
    "\n",
    "# ðŸ“Œ Load Sink Candidate Nodes from Excel\n",
    "df_ejs = pd.read_excel(ejs_data_path, sheet_name=\"Sheet1\", engine=\"openpyxl\")\n",
    "\n",
    "# ðŸ“Œ Load CO2 Source Nodes\n",
    "gdf_source = gpd.read_file(source_data_path)\n",
    "\n",
    "# Ensure 'GHG_QUAN_5' exists\n",
    "if 'GHG_QUAN_5' not in gdf_source.columns:\n",
    "    raise ValueError(\"âš  'GHG_QUAN_5' column missing in source dataset!\")\n",
    "\n",
    "# Rename columns\n",
    "df_ejs.rename(columns={'ID': 'SinkCandidateID'}, inplace=True)\n",
    "gdf_source.rename(columns={'GHGRP_ID': 'SourceID', 'GHG_QUAN_5': 'TonsCO2e'}, inplace=True)\n",
    "\n",
    "# Ensure Geometry column exists and extract Latitude/Longitude\n",
    "gdf_source = gdf_source.to_crs(epsg=4326)\n",
    "gdf_source['Latitude'] = gdf_source.geometry.y\n",
    "gdf_source['Longitude'] = gdf_source.geometry.x\n",
    "\n",
    "# Drop missing lat/lon values\n",
    "df_ejs = df_ejs.dropna(subset=['Latitude', 'Longitude'])\n",
    "gdf_source = gdf_source.dropna(subset=['Latitude', 'Longitude', 'TonsCO2e'])\n",
    "\n",
    "# Ensure lat/lon values are valid\n",
    "df_ejs['Latitude'] = df_ejs['Latitude'].clip(lower=-90, upper=90)\n",
    "df_ejs['Longitude'] = df_ejs['Longitude'].clip(lower=-180, upper=180)\n",
    "\n",
    "# Create an edge list\n",
    "edge_list = []\n",
    "\n",
    "# Compute distances\n",
    "for (sink_id, lat1, lon1), (source_id, lat2, lon2, tons_co2e) in product(\n",
    "    df_ejs[['SinkCandidateID', 'Latitude', 'Longitude']].values,\n",
    "    gdf_source[['SourceID', 'Latitude', 'Longitude', 'TonsCO2e']].values\n",
    "):\n",
    "    distance_km = geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "    if distance_km <= 160.93:  # 100 miles in km\n",
    "        edge_list.append([sink_id, source_id, distance_km, tons_co2e, \"Source\"])\n",
    "\n",
    "# Convert to DataFrame\n",
    "edge_df = pd.DataFrame(edge_list, columns=[\"From\", \"To\", \"Dist_Km\", \"TonsCO2e\", \"Type\"])\n",
    "\n",
    "# Save to CSV\n",
    "edge_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3292fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sink/candidate data...\n",
      "Loading pipeline shapefile...\n",
      "Sink candidate columns: ['ID', 'Latitude', 'Longitude', 'Class', 'Otherwell_density', 'Refwell_density', 'Refwell_DISTm', 'CrA_Presence', 'CrA_DISTm', 'PAD_Presence', 'PAD_DISTm', 'PAD_CAT_1', 'PAD_CAT_2', 'GW_DISTm', 'GW_AquiferDes', 'GW_DEPTHft', 'GW_AquiferName', 'WL_Presence', 'WL_DISTm', 'WL_Type', 'EJS_TotPop', 'EJS_TractID', 'EJS_TractLandm', 'EJS_TractWaterm', 'EJS_OZONE', 'EJS_DIESEL', 'EJS_ToxicAIR', 'EJS_TRAFFIC', 'EJS_LEADPaint', 'EJS_Superfund', 'EJS_RMP', 'EJS_HAZWaste', 'EJS_UnderTanks', 'EJS_WasteWater', 'EJS_NO2', 'EJS_DrinkWater', 'NHD_Presence', 'NHD_DISTm', 'NHD_WBType_1', 'NHD_WBType_2', 'AUS_Presence', 'AUS_DISTm', 'AUS_Name_1', 'PAR_Presence', 'PAR_OWNType', 'PAR_SubOWNType', 'PAR_Zone', 'PAR_ZoneDesc', 'PAR_STRClass', 'PAR_STRType', 'PAR_Areaft', 'COAST_Presence', 'Dep_Frio_Delta_Presence', 'Dep_Frio_Areakm2', 'Dep_Frio_LayerName', 'Dep_LowerMio1_Delta_Presence', 'Dep_LowerMio1_Areakm2', 'Dep_LowerMio1_LayerName', 'Dep_LowerMio2_Delta_Presence', 'Dep_LowerMio2_Areakm2', 'Dep_LowerMio2_LayerName', 'Dep_MiddleMio_Delta_Presence', 'Dep_MiddleMio_Areakm2', 'Dep_MiddleMio_LayerName', 'Dep_LowerWil_Delta_Presence', 'Dep_LowerWil_Areakm2', 'Dep_LowerWil_LayerName', 'Dep_MiddleWil_Delta_Presence', 'Dep_MiddleWil_Areakm2', 'Dep_MiddleWil_LayerName', 'Dep_UpperWil_Delta_Presence', 'Dep_UpperWil_Areakm2', 'Dep_UpperWil_LayerName', 'SaltDome_Presence', 'SaltDome_Aream', 'BufSaltDome_DIAMm', 'BufSaltDome_Presence', 'SurFaults_Presence', 'AccumField_LowerMio_Presence', 'AccumField_LowerMio_Aream', 'AccumField_LowerMio_DISTm', 'AccumField_MiddleMio_Presence', 'AccumField_MiddleMio_Aream', 'AccumField_MiddleMio_DISTm', 'AccumField_MiddleFrio_Presence', 'AccumField_MiddleFrio_Aream', 'AccumField_MiddleFrio_DISTm', 'AccumField_UpperFrio_Presence', 'AccumField_UpperFrio_Aream', 'AccumField_UpperFrio_DISTm', 'AccumField_Wilcox_Presence', 'AccumField_Wilcox_Aream', 'AccumField_Wilcox_DISTm', 'Co2Pipe_DISTm', 'CO2Source_DISTm', 'CO2Source_nearTonsCO2e', 'CO2Source_density', 'CO2Source_SumDensTons', 'Tribal_DISTm', 'Pet_ResDISTm', 'BRFaults_Presence', 'ClassIwell_Presence', 'USDW_valuem', 'USDW_meanm', 'TopGeo_value', 'TopGeo_mean', 'Geo_complex_score']\n",
      "Pipeline columns: ['ID', 'DIAMETER', 'LENGTH_MIL', 'CONSTRUCTI', 'Operator', 'geometry']\n",
      "Calculating distances between sink nodes and pipeline nodes...\n",
      "Progress: 5.0% (4209/84188)\n",
      "Progress: 10.0% (8418/84188)\n",
      "Progress: 15.0% (12627/84188)\n",
      "Progress: 20.0% (16836/84188)\n",
      "Progress: 25.0% (21045/84188)\n",
      "Progress: 30.0% (25254/84188)\n",
      "Progress: 35.0% (29463/84188)\n",
      "Progress: 40.0% (33672/84188)\n",
      "Progress: 45.0% (37881/84188)\n",
      "Progress: 50.0% (42090/84188)\n",
      "Progress: 55.0% (46299/84188)\n",
      "Progress: 60.0% (50508/84188)\n",
      "Progress: 65.0% (54717/84188)\n",
      "Progress: 70.0% (58926/84188)\n",
      "Progress: 75.0% (63135/84188)\n",
      "Progress: 80.0% (67344/84188)\n",
      "Progress: 85.0% (71553/84188)\n",
      "Progress: 90.0% (75762/84188)\n",
      "Progress: 95.0% (79971/84188)\n",
      "Progress: 100.0% (84180/84188)\n",
      "Creating edge list...\n",
      "ðŸŽ¯ Analysis complete. 84188 connections found between sink nodes and pipeline nodes.\n",
      "ðŸ“ Edge list saved to: C:\\Users\\himu1\\Desktop\\project\\Project2\\sink_pipeline_edge_list.csv\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None  # suppress SettingWithCopyWarning\n",
    "\n",
    "# Define file paths (update these with your correct paths)\n",
    "sink_candidate_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Output_Geospatial_Analysis_Final_filled.xlsx\"\n",
    "pipeline_shapefile_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Databases_Sreekanta\\LA_CO2Pipe.shp\"\n",
    "\n",
    "# Load the sink/candidate nodes data\n",
    "try:\n",
    "    print(\"Loading sink/candidate data...\")\n",
    "    sink_candidate_df = pd.read_excel(sink_candidate_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading excel file: {e}\")\n",
    "    sink_candidate_df = pd.read_csv(sink_candidate_path)\n",
    "\n",
    "# Load the pipeline data as GeoDataFrame\n",
    "print(\"Loading pipeline shapefile...\")\n",
    "pipeline_gdf = gpd.read_file(pipeline_shapefile_path)\n",
    "\n",
    "# Print column information for debugging\n",
    "print(\"Sink candidate columns:\", sink_candidate_df.columns.tolist())\n",
    "print(\"Pipeline columns:\", pipeline_gdf.columns.tolist())\n",
    "\n",
    "# Ensure required columns exist in pipeline data\n",
    "required_pipeline_columns = {'ID', 'DIAMETER', 'LENGTH_MIL', 'geometry'}\n",
    "missing_pipeline_columns = required_pipeline_columns - set(pipeline_gdf.columns)\n",
    "\n",
    "if missing_pipeline_columns:\n",
    "    raise ValueError(f\"âš  Missing columns in pipeline dataset: {missing_pipeline_columns}\")\n",
    "\n",
    "# Ensure required columns exist in sink/candidate data\n",
    "if 'Latitude' not in sink_candidate_df.columns or 'Longitude' not in sink_candidate_df.columns:\n",
    "    raise ValueError(f\"âš  Missing 'Latitude' or 'Longitude' columns in sink candidate dataset!\")\n",
    "\n",
    "# Process sink/candidate data\n",
    "sink_candidate_df = sink_candidate_df.dropna(subset=['Latitude', 'Longitude'])\n",
    "geometry = [Point(xy) for xy in zip(sink_candidate_df['Longitude'], sink_candidate_df['Latitude'])]\n",
    "sink_candidate_gdf = gpd.GeoDataFrame(sink_candidate_df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Ensure 'ID' column exists in sink candidate data\n",
    "if 'ID' in sink_candidate_gdf.columns:\n",
    "    sink_candidate_gdf['From'] = sink_candidate_gdf['ID']\n",
    "else:\n",
    "    sink_candidate_gdf['From'] = [f\"Sink_{i}\" for i in range(len(sink_candidate_gdf))]\n",
    "\n",
    "# Ensure CRS consistency\n",
    "if sink_candidate_gdf.crs != pipeline_gdf.crs:\n",
    "    sink_candidate_gdf = sink_candidate_gdf.to_crs(pipeline_gdf.crs)\n",
    "\n",
    "# Function to calculate distance in kilometers to a linestring (pipeline)\n",
    "def calculate_distance_to_line_km(point, line):\n",
    "    if not isinstance(point, gpd.GeoSeries):\n",
    "        point = gpd.GeoSeries([point], crs=sink_candidate_gdf.crs)\n",
    "    if not isinstance(line, gpd.GeoSeries):\n",
    "        line = gpd.GeoSeries([line], crs=pipeline_gdf.crs)\n",
    "\n",
    "    if point.crs.is_geographic:\n",
    "        point = point.to_crs(epsg=3857)  # Web Mercator projection\n",
    "        line = line.to_crs(epsg=3857)\n",
    "\n",
    "    return point.distance(line).values[0] / 1000  # Convert meters to km\n",
    "\n",
    "# Create empty lists to store edge data\n",
    "from_nodes = []\n",
    "to_nodes = []\n",
    "distances_km = []\n",
    "pipe_diameters = []\n",
    "pipe_lengths = []\n",
    "\n",
    "# For each sink/candidate node, find distance to all pipeline nodes\n",
    "print(\"Calculating distances between sink nodes and pipeline nodes...\")\n",
    "total_count = len(sink_candidate_gdf) * len(pipeline_gdf)\n",
    "count = 0\n",
    "progress_interval = max(1, total_count // 20)  # Show progress at 5% intervals\n",
    "\n",
    "for idx, sink in sink_candidate_gdf.iterrows():\n",
    "    sink_id = sink['From']\n",
    "    sink_point = sink.geometry\n",
    "    \n",
    "    for pipe_idx, pipeline in pipeline_gdf.iterrows():\n",
    "        count += 1\n",
    "        if count % progress_interval == 0:\n",
    "            print(f\"Progress: {count/total_count*100:.1f}% ({count}/{total_count})\")\n",
    "            \n",
    "        pipe_id = pipeline['ID']\n",
    "        pipe_geometry = pipeline.geometry\n",
    "        pipe_diameter = pipeline['DIAMETER']\n",
    "        pipe_length = pipeline['LENGTH_MIL']\n",
    "\n",
    "        try:\n",
    "            # Calculate distance to pipeline\n",
    "            dist_km = calculate_distance_to_line_km(sink_point, pipe_geometry)\n",
    "            \n",
    "            # Store the data\n",
    "            from_nodes.append(sink_id)\n",
    "            to_nodes.append(pipe_id)\n",
    "            distances_km.append(dist_km)\n",
    "            pipe_diameters.append(pipe_diameter)\n",
    "            pipe_lengths.append(pipe_length)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating distance between {sink_id} and {pipe_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Create edge list dataframe\n",
    "print(\"Creating edge list...\")\n",
    "edge_data = {\n",
    "    'From': from_nodes,\n",
    "    'To': to_nodes,\n",
    "    'Relational process description': 'Sink/candidate node to pipeline node',\n",
    "    'Edge Attribute': 'Distance, Diameter, Length',\n",
    "    'Column Names': 'Dist_Km, Pipe_Diam, Pipe_Length',\n",
    "    'Type': 'Pipeline',\n",
    "    'Dist_Km': distances_km,\n",
    "    'Pipe_Diam': pipe_diameters,\n",
    "    'Pipe_Length': pipe_lengths\n",
    "}\n",
    "\n",
    "edge_list_df = pd.DataFrame(edge_data)\n",
    "\n",
    "# Save the edge list to a CSV file\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\sink_pipeline_edge_list.csv\"\n",
    "edge_list_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display completion message\n",
    "print(f\"ðŸŽ¯ Analysis complete. {len(edge_list_df)} connections found between sink nodes and pipeline nodes.\")\n",
    "print(f\"ðŸ“ Edge list saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2983f04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Sink.csv\n",
      "  Columns: ['From', 'To', 'Dist_Km', 'Type']\n",
      "  Rows: 380\n",
      "Processing file: Candidate.csv\n",
      "  Columns: ['From', 'To', 'Dist_Km', 'Type']\n",
      "  Rows: 63200\n",
      "Processing file: Population.csv\n",
      "  Columns: ['From', 'To', 'Dist_Km', 'Type', 'Tract_Densqmi', 'TotPop']\n",
      "  Rows: 109609\n",
      "  Additional columns found: ['Tract_Densqmi', 'TotPop']\n",
      "Processing file: Source.csv\n",
      "  Columns: ['From', 'To', 'Dist_Km', 'Type', 'TonsCO2']\n",
      "  Rows: 266420\n",
      "  Additional columns found: ['TonsCO2']\n",
      "Processing file: Pipeline.csv\n",
      "  Columns: ['From', 'To', 'Dist_Km', 'Type', 'Pipe_Diam', 'Pipe_Length']\n",
      "  Rows: 84188\n",
      "  Additional columns found: ['Pipe_Diam', 'Pipe_Length']\n",
      "\n",
      "Merged DataFrame:\n",
      "  Total rows: 523797\n",
      "  All columns: ['From', 'To', 'Dist_Km', 'Type', 'Tract_Densqmi', 'TotPop', 'TonsCO2', 'Pipe_Diam', 'Pipe_Length']\n",
      "\n",
      "Merged edge list saved to: C:\\Users\\himu1\\Desktop\\project\\Project2\\Edge list\\merged_edge_list.csv\n",
      "\n",
      "Sample of merged data:\n",
      "     From      To     Dist_Km  Type  Tract_Densqmi TotPop TonsCO2 Pipe_Diam  \\\n",
      "0  NETL35  NETL26   27.128365  Sink            NaN   <NA>    <NA>      <NA>   \n",
      "1  NETL26  NETL35   27.128365  Sink            NaN   <NA>    <NA>      <NA>   \n",
      "2  NETL35  NETL68  192.036881  Sink            NaN   <NA>    <NA>      <NA>   \n",
      "3  NETL68  NETL35  192.036881  Sink            NaN   <NA>    <NA>      <NA>   \n",
      "4  NETL35  NETL41  179.795791  Sink            NaN   <NA>    <NA>      <NA>   \n",
      "\n",
      "   Pipe_Length  \n",
      "0          NaN  \n",
      "1          NaN  \n",
      "2          NaN  \n",
      "3          NaN  \n",
      "4          NaN  \n",
      "\n",
      "Rows by Type:\n",
      "  Source: 266420\n",
      "  PopulationCluster: 109609\n",
      "  Pipeline: 84188\n",
      "  Candidate: 63200\n",
      "  Sink: 380\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None  # suppress SettingWithCopyWarning\n",
    "\n",
    "# Define file paths\n",
    "base_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Edge list\"\n",
    "output_file_path = os.path.join(base_path, \"merged_edge_list.csv\")\n",
    "\n",
    "# Define the order of files to process\n",
    "file_types = [\"Sink\", \"Candidate\", \"Population\", \"Source\", \"Pipeline\"]\n",
    "\n",
    "# Initialize an empty DataFrame to store the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Process each file type in the specified order\n",
    "for file_type in file_types:\n",
    "    # Find files that match the current type\n",
    "    pattern = os.path.join(base_path, f\"*{file_type}*.csv\")\n",
    "    matching_files = glob.glob(pattern)\n",
    "    \n",
    "    if not matching_files:\n",
    "        print(f\"Warning: No files found matching pattern '{pattern}'\")\n",
    "        continue\n",
    "    \n",
    "    # Use the first matching file\n",
    "    file_path = matching_files[0]\n",
    "    print(f\"Processing file: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the current file\n",
    "        current_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Display basic information about the file\n",
    "        print(f\"  Columns: {current_df.columns.tolist()}\")\n",
    "        print(f\"  Rows: {len(current_df)}\")\n",
    "        \n",
    "        # Keep track of any additional columns in this file\n",
    "        additional_columns = [col for col in current_df.columns if col not in [\"From\", \"To\", \"Dist_Km\", \"Type\"]]\n",
    "        \n",
    "        if additional_columns:\n",
    "            print(f\"  Additional columns found: {additional_columns}\")\n",
    "        \n",
    "        # If this is the first file, use it to initialize the merged DataFrame\n",
    "        if merged_df.empty:\n",
    "            merged_df = current_df.copy()\n",
    "        else:\n",
    "            # Get all columns from both DataFrames\n",
    "            all_columns = set(merged_df.columns) | set(current_df.columns)\n",
    "            \n",
    "            # For each DataFrame, add missing columns with NA values\n",
    "            for col in all_columns:\n",
    "                if col not in merged_df.columns:\n",
    "                    merged_df[col] = pd.NA\n",
    "                if col not in current_df.columns:\n",
    "                    current_df[col] = pd.NA\n",
    "            \n",
    "            # Append the current DataFrame to the merged one\n",
    "            merged_df = pd.concat([merged_df, current_df], ignore_index=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Display information about the merged DataFrame\n",
    "print(\"\\nMerged DataFrame:\")\n",
    "print(f\"  Total rows: {len(merged_df)}\")\n",
    "print(f\"  All columns: {merged_df.columns.tolist()}\")\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_df.to_csv(output_file_path, index=False)\n",
    "print(f\"\\nMerged edge list saved to: {output_file_path}\")\n",
    "\n",
    "# Display a sample of the merged data\n",
    "print(\"\\nSample of merged data:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Optional: Display counts by Type for verification\n",
    "type_counts = merged_df['Type'].value_counts()\n",
    "print(\"\\nRows by Type:\")\n",
    "for type_name, count in type_counts.items():\n",
    "    print(f\"  {type_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a859e523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
