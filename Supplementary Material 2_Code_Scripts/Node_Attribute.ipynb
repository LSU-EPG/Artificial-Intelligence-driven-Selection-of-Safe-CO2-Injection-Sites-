{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from itertools import combinations\n",
    "\n",
    "# Update the file path to match your local directory\n",
    "file_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Database_with_Spatial_Analysis_final.xlsx\"\n",
    "\n",
    "# Load the dataset (modify sheet name if necessary)\n",
    "df = pd.read_excel(file_path, sheet_name=\"Sheet1\")  # Change sheet name if needed\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = {'ID', 'Latitude', 'Longitude'}\n",
    "if not required_columns.issubset(df.columns):\n",
    "    raise ValueError(f\"Missing required columns: {required_columns - set(df.columns)}\")\n",
    "\n",
    "print(\"Columns found. Proceeding with distance calculations...\")\n",
    "\n",
    "# Create an edge list\n",
    "edge_list = []\n",
    "\n",
    "# Compute all-to-all distances\n",
    "for (id1, lat1, lon1), (id2, lat2, lon2) in combinations(df[['ID', 'Latitude', 'Longitude']].values, 2):\n",
    "    distance_km = geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "    edge_list.append([id1, id2, distance_km])\n",
    "    edge_list.append([id2, id1, distance_km])  # Add reverse direction\n",
    "\n",
    "# Convert to DataFrame\n",
    "edge_df = pd.DataFrame(edge_list, columns=[\"From\", \"To\", \"Dist_Km\"])\n",
    "\n",
    "# Save to CSV\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_node_edgelist.csv\"\n",
    "edge_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Edge list with distances saved as '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe77499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge list with distances saved as 'C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_candidate_edgelist.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "from itertools import product\n",
    "\n",
    "# File paths\n",
    "sink_nodes_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Database_with_Spatial_Analysis_final.xlsx\"\n",
    "candidate_nodes_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project1\\Databases_Sreekanta\\Databases_Sreekanta\\random_points1000.shp\"\n",
    "class_ii_shut_in_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Databases_Sreekanta\\LA_NETL_SONRIS_VI_Dry.shp\"\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_candidate_edgelist.csv\"\n",
    "\n",
    "# Load sink nodes from Excel\n",
    "df_sinks = pd.read_excel(sink_nodes_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "# Load candidate nodes from shapefiles\n",
    "gdf_candidates_1 = gpd.read_file(candidate_nodes_path)  # Random points\n",
    "gdf_candidates_2 = gpd.read_file(class_ii_shut_in_path)  # Class II Shut-in Dry Hole\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns = {'ID', 'Latitude', 'Longitude'}\n",
    "if not required_columns.issubset(df_sinks.columns):\n",
    "    raise ValueError(f\"Missing columns in sink dataset: {required_columns - set(df_sinks.columns)}\")\n",
    "if 'geometry' not in gdf_candidates_1.columns or 'geometry' not in gdf_candidates_2.columns:\n",
    "    raise ValueError(\"Missing geometry column in candidate shapefiles\")\n",
    "\n",
    "# Extract latitude and longitude from geometry\n",
    "gdf_candidates_1 = gdf_candidates_1.to_crs(epsg=4326)\n",
    "gdf_candidates_2 = gdf_candidates_2.to_crs(epsg=4326)\n",
    "\n",
    "gdf_candidates_1['Latitude'] = gdf_candidates_1.geometry.y\n",
    "gdf_candidates_1['Longitude'] = gdf_candidates_1.geometry.x\n",
    "gdf_candidates_2['Latitude'] = gdf_candidates_2.geometry.y\n",
    "gdf_candidates_2['Longitude'] = gdf_candidates_2.geometry.x\n",
    "\n",
    "# Use latitude from sink_nodes_path for consistency\n",
    "df_candidates = pd.concat([\n",
    "    gdf_candidates_1[['ID', 'Latitude', 'Longitude']],\n",
    "    gdf_candidates_2[['ID', 'Latitude', 'Longitude']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# Ensure latitude values are within valid range\n",
    "df_candidates = df_candidates[(df_candidates['Latitude'] >= -90) & (df_candidates['Latitude'] <= 90)]\n",
    "\n",
    "df_sinks = df_sinks[(df_sinks['Latitude'] >= -90) & (df_sinks['Latitude'] <= 90)]\n",
    "\n",
    "# Create an edge list\n",
    "edge_list = []\n",
    "\n",
    "# Compute distances from each sink to each candidate\n",
    "for (sink_id, sink_lat, sink_lon), (cand_id, cand_lat, cand_lon) in product(\n",
    "    df_sinks[['ID', 'Latitude', 'Longitude']].values,\n",
    "    df_candidates[['ID', 'Latitude', 'Longitude']].values\n",
    "):\n",
    "    distance_km = geodesic((sink_lat, sink_lon), (cand_lat, cand_lon)).kilometers\n",
    "    edge_list.append([sink_id, cand_id, distance_km, \"Candidate\"])\n",
    "\n",
    "# Convert to DataFrame\n",
    "edge_df = pd.DataFrame(edge_list, columns=[\"From\", \"To\", \"Dist_Km\", \"Type\"])\n",
    "\n",
    "# Save to CSV\n",
    "edge_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Edge list with distances saved as '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6452b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from itertools import product\n",
    "\n",
    "# File paths\n",
    "sink_candidate_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_candidate_edgelist.csv\"\n",
    "ejs_data_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Database_with_Spatial_Analysis_final.xlsx\"\n",
    "sink_nodes_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Database_with_Spatial_Analysis_final.xlsx\"\n",
    "candidate_nodes_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project1\\Databases_Sreekanta\\Databases_Sreekanta\\random_points1000.shp\"\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_candidate_population_edgelist.csv\"\n",
    "\n",
    "# Load sink + candidate node distances\n",
    "df_sinks_candidates = pd.read_csv(sink_candidate_path)\n",
    "\n",
    "# Load sink nodes\n",
    "df_sinks = pd.read_excel(sink_nodes_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "# Load EJS data from Excel\n",
    "df_ejs = pd.read_excel(ejs_data_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "# Print available columns to debug column names\n",
    "print(\"Available columns in EJS dataset:\", df_ejs.columns)\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns_xlsx = {'TractID', 'Latitude', 'Longitude', 'TractLandm', 'TotPop'}\n",
    "if not required_columns_xlsx.issubset(df_ejs.columns):\n",
    "    raise ValueError(f\"Missing columns in EJS dataset: {required_columns_xlsx - set(df_ejs.columns)}\")\n",
    "\n",
    "# Merge latitude and longitude into sink-candidate dataset\n",
    "df_sinks_candidates = df_sinks_candidates.merge(df_sinks[['ID', 'Latitude', 'Longitude']], left_on='From', right_on='ID', how='left')\n",
    "\n",
    "# Filter population nodes within 30 miles\n",
    "def within_30_miles(lat1, lon1, lat2, lon2):\n",
    "    return geodesic((lat1, lon1), (lat2, lon2)).miles <= 30\n",
    "\n",
    "# Create an edge list\n",
    "edge_list = []\n",
    "\n",
    "# Compute distances from each sink/candidate to population node\n",
    "for (node_id, node_lat, node_lon), (pop_id, pop_lat, pop_lon, tract_land, tot_pop) in product(\n",
    "    df_sinks_candidates[['From', 'Latitude', 'Longitude']].values,\n",
    "    df_ejs[['TractID', 'Latitude', 'Longitude', 'TractLandm', 'TotPop']].values\n",
    "):\n",
    "    if within_30_miles(node_lat, node_lon, pop_lat, pop_lon):\n",
    "        distance_km = geodesic((node_lat, node_lon), (pop_lat, pop_lon)).kilometers\n",
    "        edge_list.append([node_id, pop_id, distance_km, tract_land, tot_pop])\n",
    "\n",
    "# Convert to DataFrame\n",
    "edge_df = pd.DataFrame(edge_list, columns=[\"From\", \"To\", \"Dist_Km\", \"TractLandm\", \"TotPop\"])\n",
    "\n",
    "# Save to CSV\n",
    "edge_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Edge list with population distances saved as '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from itertools import product\n",
    "\n",
    "# File paths\n",
    "ejs_data_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Database_with_Spatial_Analysis_final.xlsx\"\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_candidate_population_edgelist.csv\"\n",
    "\n",
    "# Load EJS data from Excel\n",
    "df_ejs = pd.read_excel(ejs_data_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "# Print available columns to debug column names\n",
    "print(\"Available columns in EJS dataset:\", df_ejs.columns)\n",
    "\n",
    "# Rename columns to match expected names\n",
    "df_ejs.rename(columns={\n",
    "    'EJS_TractID': 'TractID',\n",
    "    'EJS_TractLandm': 'TractLandm',\n",
    "    'EJS_TotPop': 'TotPop'\n",
    "}, inplace=True)\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns_xlsx = {'ID', 'Latitude', 'Longitude', 'TractID', 'TractLandm', 'TotPop'}\n",
    "if not required_columns_xlsx.issubset(df_ejs.columns):\n",
    "    raise ValueError(f\"Missing columns in EJS dataset: {required_columns_xlsx - set(df_ejs.columns)}\")\n",
    "\n",
    "# Create an edge list\n",
    "edge_list = []\n",
    "\n",
    "# Compute distances from each sink/candidate node (ID) to each census tract (TractID) within 30 miles\n",
    "for (id1, lat1, lon1), (tract_id, lat2, lon2, tract_land, tot_pop) in product(\n",
    "    df_ejs[['ID', 'Latitude', 'Longitude']].values,\n",
    "    df_ejs[['TractID', 'Latitude', 'Longitude', 'TractLandm', 'TotPop']].values\n",
    "):\n",
    "    distance_km = geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "    if distance_km <= 48.28:  # 30 miles in kilometers\n",
    "        edge_list.append([id1, tract_id, distance_km, tract_land, tot_pop])\n",
    "\n",
    "# Convert to DataFrame\n",
    "edge_df = pd.DataFrame(edge_list, columns=[\"From\", \"To\", \"Dist_Km\", \"TractLandm\", \"TotPop\"])\n",
    "\n",
    "# Save to CSV\n",
    "edge_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Edge list with computed distances saved as '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ca4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# File paths\n",
    "ejs_data_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Database_with_Spatial_Analysis_final.xlsx\"\n",
    "source_data_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Databases_Sreekanta\\LA_CO2_Source.shp\"\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_candidate_source_edgelist.csv\"\n",
    "\n",
    "try:\n",
    "    # Load EJS data (From locations)\n",
    "    print(\"Loading From location data...\")\n",
    "    df_ejs = pd.read_excel(ejs_data_path, sheet_name=\"Sheet1\", engine=\"openpyxl\")\n",
    "    print(f\"From location data loaded: {len(df_ejs)} rows\")\n",
    "    \n",
    "    # Load Source Nodes data (To locations)\n",
    "    print(\"Loading To location data...\")\n",
    "    gdf_source = gpd.read_file(source_data_path)\n",
    "    print(f\"To location data loaded: {len(gdf_source)} rows\")\n",
    "    \n",
    "    # Print column names to identify the correct ones\n",
    "    print(\"From location columns:\", df_ejs.columns.tolist())\n",
    "    print(\"To location columns:\", gdf_source.columns.tolist())\n",
    "    \n",
    "    # First determine if required columns exist - either directly or needing renaming\n",
    "    # For From locations\n",
    "    id_column = 'ID' if 'ID' in df_ejs.columns else None\n",
    "    if not id_column and 'From' in df_ejs.columns:\n",
    "        id_column = 'From'\n",
    "    \n",
    "    lat_column_ejs = 'Latitude' if 'Latitude' in df_ejs.columns else None\n",
    "    lon_column_ejs = 'Longitude' if 'Longitude' in df_ejs.columns else None\n",
    "    \n",
    "    # For To locations\n",
    "    source_id_column = None\n",
    "    for possible_name in ['GHGRP_ID', 'To', 'ID', 'FID']:\n",
    "        if possible_name in gdf_source.columns:\n",
    "            source_id_column = possible_name\n",
    "            break\n",
    "    \n",
    "    co2_column = None\n",
    "    for possible_name in ['GHG_QUANTI', 'TonsCO2e', 'CO2', 'Emissions']:\n",
    "        if possible_name in gdf_source.columns:\n",
    "            co2_column = possible_name\n",
    "            break\n",
    "    \n",
    "    # Validate we found all required columns\n",
    "    missing_columns = []\n",
    "    if not id_column:\n",
    "        missing_columns.append(\"ID/From column in From location data\")\n",
    "    if not lat_column_ejs:\n",
    "        missing_columns.append(\"Latitude column in From location data\")\n",
    "    if not lon_column_ejs:\n",
    "        missing_columns.append(\"Longitude column in From location data\")\n",
    "    if not source_id_column:\n",
    "        missing_columns.append(\"ID/To column in To location data\")\n",
    "    if not co2_column:\n",
    "        missing_columns.append(\"CO2 emissions column in To location data\")\n",
    "    \n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "    \n",
    "    # Create standardized column names\n",
    "    df_ejs = df_ejs.rename(columns={id_column: 'From'})\n",
    "    gdf_source = gdf_source.rename(columns={source_id_column: 'To', co2_column: 'TonsCO2e'})\n",
    "    \n",
    "    # Make sure coordinates are numeric\n",
    "    df_ejs[lat_column_ejs] = pd.to_numeric(df_ejs[lat_column_ejs], errors='coerce')\n",
    "    df_ejs[lon_column_ejs] = pd.to_numeric(df_ejs[lon_column_ejs], errors='coerce')\n",
    "    \n",
    "    # Drop rows with missing coordinates\n",
    "    before_drop = len(df_ejs)\n",
    "    df_ejs.dropna(subset=[lat_column_ejs, lon_column_ejs], inplace=True)\n",
    "    after_drop = len(df_ejs)\n",
    "    print(f\"Dropped {before_drop - after_drop} rows with missing coordinates in From location data\")\n",
    "    \n",
    "    # Calculate centroids for To locations\n",
    "    print(\"Calculating centroids for To locations...\")\n",
    "    # If geometry is not already Point type, calculate centroids\n",
    "    if not all(isinstance(geom, Point) for geom in gdf_source.geometry):\n",
    "        gdf_source['centroid'] = gdf_source.geometry.centroid\n",
    "        gdf_source['centroid_lat'] = gdf_source.centroid.y\n",
    "        gdf_source['centroid_lon'] = gdf_source.centroid.x\n",
    "    else:\n",
    "        # If already Point type, just extract coordinates\n",
    "        gdf_source['centroid_lat'] = gdf_source.geometry.y\n",
    "        gdf_source['centroid_lon'] = gdf_source.geometry.x\n",
    "    \n",
    "    # Create the edge list with distances\n",
    "    print(\"Creating edge list with distances...\")\n",
    "    edge_list = []\n",
    "    \n",
    "    # Total combinations to process\n",
    "    total_pairs = len(df_ejs) * len(gdf_source)\n",
    "    print(f\"Processing {total_pairs} connections...\")\n",
    "    \n",
    "    # Calculate distances and build edge list\n",
    "    pairs_processed = 0\n",
    "    \n",
    "    # Process in chunks to show progress\n",
    "    chunk_size = max(1, min(10000, total_pairs // 10))\n",
    "    \n",
    "    for i, sink_row in df_ejs.iterrows():\n",
    "        sink_id = sink_row['From']\n",
    "        sink_lat = sink_row[lat_column_ejs]\n",
    "        sink_lon = sink_row[lon_column_ejs]\n",
    "        \n",
    "        # Skip if missing coordinate data\n",
    "        if pd.isna(sink_lat) or pd.isna(sink_lon):\n",
    "            continue\n",
    "        \n",
    "        for j, source_row in gdf_source.iterrows():\n",
    "            pairs_processed += 1\n",
    "            \n",
    "            # Show progress\n",
    "            if pairs_processed % chunk_size == 0:\n",
    "                progress_pct = (pairs_processed / total_pairs) * 100\n",
    "                print(f\"Progress: {progress_pct:.1f}% ({pairs_processed}/{total_pairs})\")\n",
    "            \n",
    "            source_id = source_row['To']\n",
    "            source_lat = source_row['centroid_lat']\n",
    "            source_lon = source_row['centroid_lon']\n",
    "            tons_co2e = source_row['TonsCO2e']\n",
    "            \n",
    "            # Skip if missing coordinate data\n",
    "            if pd.isna(source_lat) or pd.isna(source_lon) or pd.isna(tons_co2e):\n",
    "                continue\n",
    "                \n",
    "            # Validate coordinates are in proper range\n",
    "            if (-90 <= sink_lat <= 90 and -180 <= sink_lon <= 180 and \n",
    "                -90 <= source_lat <= 90 and -180 <= source_lon <= 180):\n",
    "                \n",
    "                try:\n",
    "                    # Calculate distance\n",
    "                    distance_km = geodesic(\n",
    "                        (sink_lat, sink_lon), \n",
    "                        (source_lat, source_lon)\n",
    "                    ).kilometers\n",
    "                    \n",
    "                    # Add all connections regardless of distance\n",
    "                    edge_list.append({\n",
    "                        'From': sink_id,\n",
    "                        'To': source_id,\n",
    "                        'Dist_Km': distance_km,\n",
    "                        'TonsCO2e': tons_co2e,\n",
    "                        'Type': \"Source\"\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating distance: {e} for points ({sink_lat},{sink_lon}) and ({source_lat},{source_lon})\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    edge_df = pd.DataFrame(edge_list)\n",
    "    \n",
    "    # Check if we found any connections\n",
    "    if edge_df.empty:\n",
    "        print(\"WARNING: No connections created!\")\n",
    "        print(\"This could indicate a problem with the input data.\")\n",
    "        \n",
    "        # Create an empty CSV with the correct columns\n",
    "        empty_df = pd.DataFrame(columns=['From', 'To', 'Dist_Km', 'TonsCO2e', 'Type'])\n",
    "        empty_df.to_csv(output_path, index=False)\n",
    "        print(f\"Empty CSV with required columns created at: {output_path}\")\n",
    "    else:\n",
    "        print(f\"Created {len(edge_df)} connections\")\n",
    "        \n",
    "        # Make sure all required columns are present and in the right order\n",
    "        result_columns = ['From', 'To', 'Dist_Km', 'TonsCO2e', 'Type']\n",
    "        for col in result_columns:\n",
    "            if col not in edge_df.columns:\n",
    "                if col == 'Type':\n",
    "                    # Add Type column if missing\n",
    "                    edge_df['Type'] = 'Source'\n",
    "        \n",
    "        # Reorder columns to ensure they're in the expected order\n",
    "        edge_df = edge_df[result_columns]\n",
    "        \n",
    "        # Display information about the output\n",
    "        print(\"\\nColumns in the final CSV:\")\n",
    "        for col in edge_df.columns:\n",
    "            print(f\"- {col}: {edge_df[col].dtype}\")\n",
    "        \n",
    "        print(\"\\nSample of first 5 rows:\")\n",
    "        print(edge_df.head())\n",
    "        \n",
    "        # Distance statistics\n",
    "        print(\"\\nDistance statistics (km):\")\n",
    "        print(f\"Min: {edge_df['Dist_Km'].min():.2f}\")\n",
    "        print(f\"Max: {edge_df['Dist_Km'].max():.2f}\")\n",
    "        print(f\"Mean: {edge_df['Dist_Km'].mean():.2f}\")\n",
    "        \n",
    "        # Save to CSV - ensure all columns are exported\n",
    "        try:\n",
    "            edge_df.to_csv(output_path, index=False)\n",
    "            print(f\"\\nCSV file successfully created at: {output_path}\")\n",
    "            print(f\"File contains {len(edge_df)} rows and {len(edge_df.columns)} columns\")\n",
    "            \n",
    "            # Verify file was created\n",
    "            if os.path.exists(output_path):\n",
    "                file_size = os.path.getsize(output_path) / 1024  # Size in KB\n",
    "                print(f\"File size: {file_size:.2f} KB\")\n",
    "            else:\n",
    "                print(\"Warning: File does not appear to exist after saving!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to CSV: {e}\")\n",
    "            \n",
    "            # Try an alternative location\n",
    "            alt_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"sink_source_edgelist_backup.csv\")\n",
    "            edge_df.to_csv(alt_path, index=False)\n",
    "            print(f\"Saved to alternative location: {alt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "    \n",
    "    # Try to create a minimal CSV with the required columns even if processing failed\n",
    "    try:\n",
    "        minimal_df = pd.DataFrame(columns=['From', 'To', 'Dist_Km', 'TonsCO2e', 'Type'])\n",
    "        backup_path = os.path.join(os.path.dirname(output_path), \"empty_edge_list.csv\")\n",
    "        minimal_df.to_csv(backup_path, index=False)\n",
    "        print(f\"Created empty CSV template at: {backup_path}\")\n",
    "    except:\n",
    "        print(\"Could not create empty CSV template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e875d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "from itertools import product\n",
    "\n",
    "# File paths\n",
    "ejs_data_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Database_with_Spatial_Analysis_final.xlsx\"\n",
    "pipeline_data_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Databases_Sreekanta\\LA_CO2Pipe.shp\"\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_candidate_pipeline_edgelist.csv\"\n",
    "\n",
    "# Load EJS data from Excel using openpyxl\n",
    "df_ejs = pd.read_excel(ejs_data_path, sheet_name=\"Sheet1\", engine=\"openpyxl\")\n",
    "\n",
    "# Load Pipeline Nodes data using GeoPandas\n",
    "gdf_pipeline = gpd.read_file(pipeline_data_path)\n",
    "\n",
    "# Print available columns to debug column names\n",
    "print(\"Available columns in EJS dataset:\", df_ejs.columns)\n",
    "print(\"Available columns in Pipeline dataset:\", gdf_pipeline.columns)\n",
    "\n",
    "# Rename columns to match expected names\n",
    "df_ejs.rename(columns={'ID': 'From'}, inplace=True)\n",
    "if 'PLINE_ID' in gdf_pipeline.columns:\n",
    "    gdf_pipeline.rename(columns={'PLINE_ID': 'To'}, inplace=True)\n",
    "    gdf_pipeline['To'] = gdf_pipeline['To'].astype(str)  # Ensure To column is string\n",
    "else:\n",
    "    raise ValueError(\"PLINE_ID column not found in Pipeline dataset. Check correct column name.\")\n",
    "\n",
    "# Convert Latitude/Longitude to numeric\n",
    "df_ejs['Latitude'] = pd.to_numeric(df_ejs['Latitude'], errors='coerce')\n",
    "df_ejs['Longitude'] = pd.to_numeric(df_ejs['Longitude'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing Latitude/Longitude\n",
    "df_ejs.dropna(subset=['Latitude', 'Longitude'], inplace=True)\n",
    "\n",
    "# Extract Latitude and Longitude from geometry in pipeline dataset\n",
    "gdf_pipeline['Latitude'] = gdf_pipeline.geometry.centroid.y\n",
    "gdf_pipeline['Longitude'] = gdf_pipeline.geometry.centroid.x\n",
    "\n",
    "# Debugging Step: Print pipeline data to ensure values exist\n",
    "print(\"Sample Pipeline Data:\")\n",
    "print(gdf_pipeline[['To', 'Latitude', 'Longitude']].head())\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns_ejs = {'From', 'Latitude', 'Longitude'}\n",
    "required_columns_pipeline = {'To', 'Latitude', 'Longitude'}\n",
    "if not required_columns_ejs.issubset(df_ejs.columns):\n",
    "    raise ValueError(f\"Missing columns in EJS dataset: {required_columns_ejs - set(df_ejs.columns)}\")\n",
    "if not required_columns_pipeline.issubset(gdf_pipeline.columns):\n",
    "    raise ValueError(f\"Missing columns in Pipeline dataset: {required_columns_pipeline - set(gdf_pipeline.columns)}\")\n",
    "\n",
    "# Create an edge list\n",
    "edge_list = []\n",
    "\n",
    "# Compute distances from each sink/candidate node (From) to each pipeline node (To)\n",
    "for (sink_id, lat1, lon1), (pipeline_id, lat2, lon2) in product(\n",
    "    df_ejs[['From', 'Latitude', 'Longitude']].values,\n",
    "    gdf_pipeline[['To', 'Latitude', 'Longitude']].values\n",
    "):\n",
    "    if -90 <= lat1 <= 90 and -90 <= lat2 <= 90:\n",
    "        distance_km = geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "        edge_list.append([sink_id, pipeline_id, distance_km, \"Pipeline\"])\n",
    "\n",
    "# Convert to DataFrame\n",
    "edge_df = pd.DataFrame(edge_list, columns=[\"From\", \"To\", \"Dist_Km\", \"Type\"])\n",
    "\n",
    "# Save to CSV\n",
    "edge_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Edge list with computed distances saved as '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None  # suppress SettingWithCopyWarning\n",
    "\n",
    "# Define file paths (update these with your correct paths)\n",
    "sink_candidate_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Database_with_Spatial_Analysis_final.xlsx\"\n",
    "source_shapefile_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Databases_Sreekanta\\LA_CO2_Source.shp\"\n",
    "\n",
    "# Load the sink/candidate nodes data - corrected to use read_excel instead of read_csv\n",
    "try:\n",
    "    sink_candidate_df = pd.read_excel(sink_candidate_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading excel file: {e}\")\n",
    "    # Fallback to CSV in case the file was saved as CSV despite the .xlsx extension\n",
    "    sink_candidate_df = pd.read_csv(sink_candidate_path)\n",
    "\n",
    "# Check and use LATITUDE and LONGITUDE columns in source_gdf if geometry is not already present\n",
    "if 'geometry' not in source_gdf.columns and 'LATITUDE' in source_gdf.columns and 'LONGITUDE' in source_gdf.columns:\n",
    "    print(\"Creating geometry for source data using LATITUDE and LONGITUDE columns\")\n",
    "    source_gdf = source_gdf.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "    geometry = [Point(xy) for xy in zip(source_gdf['LONGITUDE'], source_gdf['LATITUDE'])]\n",
    "    source_gdf = gpd.GeoDataFrame(source_gdf, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Check columns to ensure they exist\n",
    "print(\"Sink candidate columns:\", sink_candidate_df.columns.tolist())\n",
    "print(\"Source columns:\", source_gdf.columns.tolist())\n",
    "\n",
    "# Make sure both dataframes have geometry columns\n",
    "# If sink_candidate_df doesn't have geometry, create it from lat/lon columns\n",
    "if 'geometry' not in sink_candidate_df.columns:\n",
    "    # Use the exact column names specified for sink_candidate_df\n",
    "    if 'Latitude' in sink_candidate_df.columns and 'Longitude' in sink_candidate_df.columns:\n",
    "        # Filter out any rows with missing coordinates\n",
    "        sink_candidate_df = sink_candidate_df.dropna(subset=['Latitude', 'Longitude'])\n",
    "        geometry = [Point(xy) for xy in zip(sink_candidate_df['Longitude'], sink_candidate_df['Latitude'])]\n",
    "        sink_candidate_gdf = gpd.GeoDataFrame(sink_candidate_df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    else:\n",
    "        raise ValueError(f\"Could not find 'Latitude' and 'Longitude' columns in sink candidate data. Available columns: {sink_candidate_df.columns.tolist()}\")\n",
    "else:\n",
    "    sink_candidate_gdf = gpd.GeoDataFrame(sink_candidate_df, crs=\"EPSG:4326\")\n",
    "\n",
    "# Ensure both GeoDataFrames use the same coordinate reference system\n",
    "print(f\"Sink CRS: {sink_candidate_gdf.crs}, Source CRS: {source_gdf.crs}\")\n",
    "if sink_candidate_gdf.crs != source_gdf.crs:\n",
    "    sink_candidate_gdf = sink_candidate_gdf.to_crs(source_gdf.crs)\n",
    "\n",
    "# Function to calculate distance in kilometers\n",
    "def calculate_distance_km(point1, point2):\n",
    "    # Make sure we're working with GeoSeries objects\n",
    "    if not isinstance(point1, gpd.GeoSeries):\n",
    "        point1 = gpd.GeoSeries([point1], crs=sink_candidate_gdf.crs)\n",
    "    if not isinstance(point2, gpd.GeoSeries):\n",
    "        point2 = gpd.GeoSeries([point2], crs=source_gdf.crs)\n",
    "    \n",
    "    # Convert to projected CRS for accurate distance calculation\n",
    "    if point1.crs.is_geographic:\n",
    "        point1 = point1.to_crs(epsg=3857)  # Web Mercator projection\n",
    "        point2 = point2.to_crs(epsg=3857)\n",
    "    \n",
    "    # Calculate distance in meters and convert to kilometers\n",
    "    distance_m = point1.distance(point2).values[0]  # Extract the value from the series\n",
    "    return distance_m / 1000\n",
    "\n",
    "# Use 'ID' column as the identifier for sink/candidate nodes\n",
    "if 'ID' in sink_candidate_gdf.columns:\n",
    "    print(\"Using 'ID' column as sink identifier\")\n",
    "    sink_candidate_gdf['From'] = sink_candidate_gdf['ID']\n",
    "else:\n",
    "    # Fallback if 'ID' column doesn't exist\n",
    "    id_col = next((col for col in sink_candidate_gdf.columns if 'id' in col.lower()), None)\n",
    "    if id_col:\n",
    "        print(f\"Using '{id_col}' as sink ID column\")\n",
    "        sink_candidate_gdf['From'] = sink_candidate_gdf[id_col]\n",
    "    else:\n",
    "        print(\"Creating sequential IDs for sinks\")\n",
    "        sink_candidate_gdf['From'] = [f\"Sink_{i}\" for i in range(len(sink_candidate_gdf))]\n",
    "\n",
    "# Check for GHGRP_ID in source data\n",
    "source_id_col = 'GHGRP_ID'\n",
    "if source_id_col not in source_gdf.columns:\n",
    "    source_id_col = next((col for col in source_gdf.columns if 'id' in col.lower()), None)\n",
    "    if not source_id_col:\n",
    "        print(\"Creating sequential IDs for sources\")\n",
    "        source_gdf['GHGRP_ID'] = [f\"Source_{i}\" for i in range(len(source_gdf))]\n",
    "        source_id_col = 'GHGRP_ID'\n",
    "    else:\n",
    "        print(f\"Using '{source_id_col}' as source ID column\")\n",
    "\n",
    "# Check for emissions column\n",
    "emission_col = 'GHG_QUANTI'\n",
    "if emission_col not in source_gdf.columns:\n",
    "    emission_col = next((col for col in source_gdf.columns if 'co2' in col.lower() or 'ghg' in col.lower() or 'emission' in col.lower() or 'quanti' in col.lower()), None)\n",
    "    if not emission_col:\n",
    "        print(\"No emissions data found, using placeholder values\")\n",
    "        source_gdf[emission_col] = 1000  # Default placeholder value\n",
    "    else:\n",
    "        print(f\"Using '{emission_col}' as emissions column\")\n",
    "else:\n",
    "    print(f\"Using '{emission_col}' as emissions column\")\n",
    "\n",
    "# Create empty lists to store edge data\n",
    "from_nodes = []\n",
    "to_nodes = []\n",
    "distances_km = []\n",
    "co2_tons = []\n",
    "\n",
    "# Define radius in kilometers (100 miles â‰ˆ 160.9344 kilometers)\n",
    "radius_km = 160.9344\n",
    "\n",
    "# For each sink/candidate node, find sources within radius\n",
    "for idx, sink in sink_candidate_gdf.iterrows():\n",
    "    sink_id = sink['From']\n",
    "    sink_point = sink.geometry\n",
    "    \n",
    "    # Find sources within radius\n",
    "    for src_idx, source in source_gdf.iterrows():\n",
    "        source_id = source[source_id_col]\n",
    "        source_point = source.geometry\n",
    "        \n",
    "        try:\n",
    "            # Calculate distance\n",
    "            dist_km = calculate_distance_km(sink_point, source_point)\n",
    "            \n",
    "            # If within radius, add to edge list\n",
    "            if dist_km <= radius_km:\n",
    "                from_nodes.append(sink_id)\n",
    "                to_nodes.append(source_id)\n",
    "                distances_km.append(dist_km)\n",
    "                co2_tons.append(source[emission_col])\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating distance between {sink_id} and {source_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Create edge list dataframe\n",
    "edge_list_df = pd.DataFrame({\n",
    "    'From': from_nodes,\n",
    "    'To': to_nodes,\n",
    "    'Relational process description': 'Sink/candidate node to source node within 100 miles',\n",
    "    'Edge Attribute': 'Distance and CO2 emissions',\n",
    "    'Column Names': 'Dist_Km, TonsCO2e',\n",
    "    'Type': 'Source',\n",
    "    'Dist_Km': distances_km,\n",
    "    'TonsCO2e': co2_tons\n",
    "})\n",
    "\n",
    "# Save the edge list to a CSV file\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_source_edge_list.csv\"\n",
    "edge_list_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the first few rows of the edge list\n",
    "print(edge_list_df.head())\n",
    "print(f\"Total connections found: {len(edge_list_df)}\")\n",
    "\n",
    "# Visualization section removed as requested\n",
    "print(\"Skipping visualization portion\")\n",
    "print(f\"Analysis complete. {len(edge_list_df)} connections found between sources and sinks.\")\n",
    "print(f\"Edge list saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a858a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None  # suppress SettingWithCopyWarning\n",
    "\n",
    "# Define file paths (update these with your correct paths)\n",
    "sink_candidate_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Database_with_Spatial_Analysis_final.xlsx\"\n",
    "pipeline_shapefile_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\Databases_Sreekanta\\LA_CO2Pipe.shp\"\n",
    "\n",
    "# Load the sink/candidate nodes data\n",
    "try:\n",
    "    print(\"Loading sink/candidate data...\")\n",
    "    sink_candidate_df = pd.read_excel(sink_candidate_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading excel file: {e}\")\n",
    "    # Fallback to CSV in case the file was saved as CSV despite the .xlsx extension\n",
    "    sink_candidate_df = pd.read_csv(sink_candidate_path)\n",
    "\n",
    "# Load the pipeline data as GeoDataFrame\n",
    "print(\"Loading pipeline shapefile...\")\n",
    "pipeline_gdf = gpd.read_file(pipeline_shapefile_path)\n",
    "\n",
    "# Print column information for debugging\n",
    "print(\"Sink candidate columns:\", sink_candidate_df.columns.tolist())\n",
    "print(\"Pipeline columns:\", pipeline_gdf.columns.tolist())\n",
    "\n",
    "# Check for FID column in pipeline data\n",
    "pipeline_id_col = 'FID'\n",
    "if pipeline_id_col not in pipeline_gdf.columns:\n",
    "    # Try to find appropriate ID column\n",
    "    pipeline_id_col = next((col for col in pipeline_gdf.columns \n",
    "                          if col.lower() in ['fid', 'id', 'pipeline_id', 'pipe_id']), None)\n",
    "    if not pipeline_id_col:\n",
    "        print(\"Creating sequential IDs for pipeline nodes\")\n",
    "        pipeline_gdf['FID'] = [f\"Pipeline_{i}\" for i in range(len(pipeline_gdf))]\n",
    "        pipeline_id_col = 'FID'\n",
    "    else:\n",
    "        print(f\"Using '{pipeline_id_col}' as pipeline ID column\")\n",
    "\n",
    "# Make sure both dataframes have geometry columns\n",
    "# Process sink/candidate data\n",
    "if 'geometry' not in sink_candidate_df.columns:\n",
    "    # Use the exact column names for sink_candidate_df\n",
    "    if 'Latitude' in sink_candidate_df.columns and 'Longitude' in sink_candidate_df.columns:\n",
    "        # Filter out any rows with missing coordinates\n",
    "        sink_candidate_df = sink_candidate_df.dropna(subset=['Latitude', 'Longitude'])\n",
    "        geometry = [Point(xy) for xy in zip(sink_candidate_df['Longitude'], sink_candidate_df['Latitude'])]\n",
    "        sink_candidate_gdf = gpd.GeoDataFrame(sink_candidate_df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    else:\n",
    "        raise ValueError(f\"Could not find 'Latitude' and 'Longitude' columns in sink candidate data. Available columns: {sink_candidate_df.columns.tolist()}\")\n",
    "else:\n",
    "    sink_candidate_gdf = gpd.GeoDataFrame(sink_candidate_df, crs=\"EPSG:4326\")\n",
    "\n",
    "# Use 'ID' column as the identifier for sink/candidate nodes\n",
    "if 'ID' in sink_candidate_gdf.columns:\n",
    "    print(\"Using 'ID' column as sink identifier\")\n",
    "    sink_candidate_gdf['From'] = sink_candidate_gdf['ID']\n",
    "else:\n",
    "    # Fallback if 'ID' column doesn't exist\n",
    "    id_col = next((col for col in sink_candidate_gdf.columns if 'id' in col.lower()), None)\n",
    "    if id_col:\n",
    "        print(f\"Using '{id_col}' as sink ID column\")\n",
    "        sink_candidate_gdf['From'] = sink_candidate_gdf[id_col]\n",
    "    else:\n",
    "        print(\"Creating sequential IDs for sinks\")\n",
    "        sink_candidate_gdf['From'] = [f\"Sink_{i}\" for i in range(len(sink_candidate_gdf))]\n",
    "\n",
    "# Ensure both GeoDataFrames use the same coordinate reference system\n",
    "print(f\"Sink CRS: {sink_candidate_gdf.crs}, Pipeline CRS: {pipeline_gdf.crs}\")\n",
    "if sink_candidate_gdf.crs != pipeline_gdf.crs:\n",
    "    sink_candidate_gdf = sink_candidate_gdf.to_crs(pipeline_gdf.crs)\n",
    "\n",
    "# Function to calculate distance in kilometers to a linestring (pipeline)\n",
    "def calculate_distance_to_line_km(point, line):\n",
    "    # Make sure we're working with GeoSeries objects\n",
    "    if not isinstance(point, gpd.GeoSeries):\n",
    "        point = gpd.GeoSeries([point], crs=sink_candidate_gdf.crs)\n",
    "    if not isinstance(line, gpd.GeoSeries):\n",
    "        line = gpd.GeoSeries([line], crs=pipeline_gdf.crs)\n",
    "    \n",
    "    # Convert to projected CRS for accurate distance calculation\n",
    "    if point.crs.is_geographic:\n",
    "        point = point.to_crs(epsg=3857)  # Web Mercator projection\n",
    "        line = line.to_crs(epsg=3857)\n",
    "    \n",
    "    # Calculate distance in meters and convert to kilometers\n",
    "    distance_m = point.distance(line).values[0]  # Extract the value from the series\n",
    "    return distance_m / 1000\n",
    "\n",
    "# Create empty lists to store edge data\n",
    "from_nodes = []\n",
    "to_nodes = []\n",
    "distances_km = []\n",
    "\n",
    "# For each sink/candidate node, find distance to all pipeline nodes\n",
    "print(\"Calculating distances between sink nodes and pipeline nodes...\")\n",
    "total_count = len(sink_candidate_gdf) * len(pipeline_gdf)\n",
    "count = 0\n",
    "progress_interval = max(1, total_count // 20)  # Show progress at 5% intervals\n",
    "\n",
    "for idx, sink in sink_candidate_gdf.iterrows():\n",
    "    sink_id = sink['From']\n",
    "    sink_point = sink.geometry\n",
    "    \n",
    "    for pipe_idx, pipeline in pipeline_gdf.iterrows():\n",
    "        count += 1\n",
    "        if count % progress_interval == 0:\n",
    "            print(f\"Progress: {count/total_count*100:.1f}% ({count}/{total_count})\")\n",
    "            \n",
    "        pipe_id = pipeline[pipeline_id_col]\n",
    "        pipe_geometry = pipeline.geometry\n",
    "        \n",
    "        try:\n",
    "            # Calculate distance to pipeline\n",
    "            dist_km = calculate_distance_to_line_km(sink_point, pipe_geometry)\n",
    "            \n",
    "            # Store the data\n",
    "            from_nodes.append(sink_id)\n",
    "            to_nodes.append(pipe_id)\n",
    "            distances_km.append(dist_km)\n",
    "            \n",
    "            # We don't need to collect Pipeline IDs as requested\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating distance between {sink_id} and {pipe_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Create edge list dataframe\n",
    "print(\"Creating edge list...\")\n",
    "edge_data = {\n",
    "    'From': from_nodes,\n",
    "    'To': to_nodes,\n",
    "    'Relational process description': 'Sink/candidate node to pipeline node',\n",
    "    'Edge Attribute': 'Distance',\n",
    "    'Column Names': 'Dist_Km',\n",
    "    'Type': 'Pipeline',\n",
    "    'Dist_Km': distances_km\n",
    "}\n",
    "\n",
    "edge_list_df = pd.DataFrame(edge_data)\n",
    "\n",
    "# Save the edge list to a CSV file\n",
    "output_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\sink_pipeline_edge_list.csv\"\n",
    "edge_list_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the first few rows of the edge list\n",
    "print(edge_list_df.head())\n",
    "print(f\"Analysis complete. {len(edge_list_df)} connections found between sink nodes and pipeline nodes.\")\n",
    "print(f\"Edge list saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None  # suppress SettingWithCopyWarning\n",
    "\n",
    "# Define file paths\n",
    "input_file_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_candidate_population_edgelist.csv\"\n",
    "output_file_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Databases_Sreekanta\\sink_candidate_population_edgelist_updated.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "try:\n",
    "    print(f\"Reading file from: {input_file_path}\")\n",
    "    edge_list_df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    # Display information about the file\n",
    "    print(f\"File loaded successfully. Shape: {edge_list_df.shape}\")\n",
    "    print(f\"Columns: {edge_list_df.columns.tolist()}\")\n",
    "    print(f\"First few rows:\\n{edge_list_df.head()}\")\n",
    "    \n",
    "    # Add the Type column with \"PopulationCluster\" value\n",
    "    edge_list_df['Type'] = 'PopulationCluster'\n",
    "    \n",
    "    # Save the updated CSV file\n",
    "    edge_list_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Updated file saved to: {output_file_path}\")\n",
    "    \n",
    "    # Show the updated data\n",
    "    print(f\"Updated first few rows:\\n{edge_list_df.head()}\")\n",
    "    print(f\"Total rows processed: {len(edge_list_df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None  # suppress SettingWithCopyWarning\n",
    "\n",
    "# Define file paths\n",
    "base_path = r\"C:\\Users\\himu1\\Desktop\\project\\Project2\\Edge list\"\n",
    "output_file_path = os.path.join(base_path, \"merged_edge_list.csv\")\n",
    "\n",
    "# Define the order of files to process\n",
    "file_types = [\"Sink\", \"Candidate\", \"Population\", \"Source\", \"Pipeline\"]\n",
    "\n",
    "# Initialize an empty DataFrame to store the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Process each file type in the specified order\n",
    "for file_type in file_types:\n",
    "    # Find files that match the current type\n",
    "    pattern = os.path.join(base_path, f\"*{file_type}*.csv\")\n",
    "    matching_files = glob.glob(pattern)\n",
    "    \n",
    "    if not matching_files:\n",
    "        print(f\"Warning: No files found matching pattern '{pattern}'\")\n",
    "        continue\n",
    "    \n",
    "    # Use the first matching file\n",
    "    file_path = matching_files[0]\n",
    "    print(f\"Processing file: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the current file\n",
    "        current_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Display basic information about the file\n",
    "        print(f\"  Columns: {current_df.columns.tolist()}\")\n",
    "        print(f\"  Rows: {len(current_df)}\")\n",
    "        \n",
    "        # Keep track of any additional columns in this file\n",
    "        additional_columns = [col for col in current_df.columns if col not in [\"From\", \"To\", \"Dist_Km\", \"Type\"]]\n",
    "        \n",
    "        if additional_columns:\n",
    "            print(f\"  Additional columns found: {additional_columns}\")\n",
    "        \n",
    "        # If this is the first file, use it to initialize the merged DataFrame\n",
    "        if merged_df.empty:\n",
    "            merged_df = current_df.copy()\n",
    "        else:\n",
    "            # Get all columns from both DataFrames\n",
    "            all_columns = set(merged_df.columns) | set(current_df.columns)\n",
    "            \n",
    "            # For each DataFrame, add missing columns with NA values\n",
    "            for col in all_columns:\n",
    "                if col not in merged_df.columns:\n",
    "                    merged_df[col] = pd.NA\n",
    "                if col not in current_df.columns:\n",
    "                    current_df[col] = pd.NA\n",
    "            \n",
    "            # Append the current DataFrame to the merged one\n",
    "            merged_df = pd.concat([merged_df, current_df], ignore_index=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Display information about the merged DataFrame\n",
    "print(\"\\nMerged DataFrame:\")\n",
    "print(f\"  Total rows: {len(merged_df)}\")\n",
    "print(f\"  All columns: {merged_df.columns.tolist()}\")\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_df.to_csv(output_file_path, index=False)\n",
    "print(f\"\\nMerged edge list saved to: {output_file_path}\")\n",
    "\n",
    "# Display a sample of the merged data\n",
    "print(\"\\nSample of merged data:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Optional: Display counts by Type for verification\n",
    "type_counts = merged_df['Type'].value_counts()\n",
    "print(\"\\nRows by Type:\")\n",
    "for type_name, count in type_counts.items():\n",
    "    print(f\"  {type_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420c88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
